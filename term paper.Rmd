---
title: "Term Paper"
author: "Chua Xin Xuan"
date: "4/22/2022"
output: html_document
---

## Web-scraping
```{r}
station_code <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/station_code.csv")
```

```{r}
# The general form of the URL of the weather data is:
# https://www.weather.gov.sg/files/dailydata/DAILYDATA_STATIONCODE_YYYYMM.csv

# For year 2021
yy <- "2018"
mths <- sprintf("%02d", 1:12)
stations <- station_code$stn_code
root_s <- "http://www.weather.gov.sg/files/dailydata/"

for(s in stations) {
  for (mm in mths) {
    uu <- paste(root_s, 
                "DAILYDATA_", s, "_",
                yy, mm, ".csv", sep ="")
    outname <- paste("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/2018/", yy, mm, "_", s, ".csv", sep ="")
    cat(outname, "\n")
    try(download.file(uu, outname))
  }
}
```

## Loop through file to combine data
```{r}
files <- list.files(path = "C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/2018/", pattern = "*.csv")

df_total = data.frame()
for (i in files) {
  uu <- paste("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/2018/", i, sep = "")
  
  temp <- read.csv(uu, skip = 1, header = FALSE, na.strings = 'â€”')
  
  df_total <- rbind(df_total, temp)
  
}
```

```{r}
# save df_total in csv
write.csv(df_total, "C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/df_2018.csv", row.names = FALSE)
```


## Reading in Data
Missing data were ommitted as there were some stations across singapore which did not have any data collected.
```{r}
df_2021 <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/df_2021.csv")

colnames(df_2021) <- c('station', 'yr', 'mon', 'day', 'total_rainfall', '30min', 
                        '60min', '120min', 'mean_temp', 'max_temp', 'min_temp',
                        'mean_windspeed', 'max_windspeed')
df_2021 <- na.omit(df_2021)
```

```{r}
df_2020 <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/df_2020.csv")

colnames(df_2020) <- c('station', 'yr', 'mon', 'day', 'total_rainfall', '30min', 
                        '60min', '120min', 'mean_temp', 'max_temp', 'min_temp',
                        'mean_windspeed', 'max_windspeed')

df_2020$total_rainfall <- as.numeric(df_2020$total_rainfall)
df_2020$mean_temp <- as.numeric(df_2020$mean_temp)
df_2020$max_temp <- as.numeric(df_2020$max_temp)
df_2020$min_temp <- as.numeric(df_2020$min_temp)
df_2020$mean_windspeed <- as.numeric(df_2020$mean_windspeed)
df_2020$max_windspeed <- as.numeric(df_2020$max_windspeed)

df_2020 <- na.omit(df_2020)
```

```{r}
df_2019 <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/df_2019.csv")

colnames(df_2019) <- c('station', 'yr', 'mon', 'day', 'total_rainfall', '30min', 
                        '60min', '120min', 'mean_temp', 'max_temp', 'min_temp',
                        'mean_windspeed', 'max_windspeed')
df_2019$mean_temp <- as.numeric(df_2019$mean_temp)
df_2019$max_temp <- as.numeric(df_2019$max_temp)
df_2019$min_temp <- as.numeric(df_2019$min_temp)
df_2019$mean_windspeed <- as.numeric(df_2019$mean_windspeed)
df_2019$max_windspeed <- as.numeric(df_2019$max_windspeed)

df_2019 <- na.omit(df_2019)
```

```{r}
df_2018 <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/data/df_2018.csv")

colnames(df_2018) <- c('station', 'yr', 'mon', 'day', 'total_rainfall', '30min', 
                        '60min', '120min', 'mean_temp', 'max_temp', 'min_temp',
                        'mean_windspeed', 'max_windspeed')
df_2018 <- na.omit(df_2018)
```


## Data pre-processing
```{r}
library(tidyverse)

df_2018_new <- df_2018 %>% group_by(mon, day) %>%
  summarise(
    total_rain = sum(total_rainfall),
    mean_rain = mean(total_rainfall),
    mean_temp = mean(mean_temp),
    max_temp = max(max_temp),
    min_temp = min(min_temp),
    mean_windspeed = mean(mean_windspeed),
    max_windspeed = max(max_windspeed)
  )
df_2018_new$year = 2018
df_2018_new$Y <- ifelse(df_2018_new$total_rain > 0, 1, 0)


df_2019_new <- df_2019 %>% group_by(mon, day) %>%
  summarise(
    total_rain = sum(total_rainfall),
    mean_rain = mean(total_rainfall),
    mean_temp = mean(mean_temp),
    max_temp = max(max_temp),
    min_temp = min(min_temp),
    mean_windspeed = mean(mean_windspeed),
    max_windspeed = max(max_windspeed)
  )
df_2019_new$year = 2019
df_2019_new$Y <- ifelse(df_2019_new$total_rain > 0, 1, 0)



df_2020_new <- df_2020 %>% group_by(mon, day) %>%
  summarise(
    total_rain = sum(total_rainfall),
    mean_rain = mean(total_rainfall),
    mean_temp = mean(mean_temp),
    max_temp = max(max_temp),
    min_temp = min(min_temp),
    mean_windspeed = mean(mean_windspeed),
    max_windspeed = max(max_windspeed)
  )
df_2020_new$year = 2020
df_2020_new$Y <- ifelse(df_2020_new$total_rain > 0, 1, 0)



df_2021_new <- df_2021 %>% group_by(mon, day) %>%
  summarise(
    total_rain = sum(total_rainfall),
    mean_rain = mean(total_rainfall),
    mean_temp = mean(mean_temp),
    max_temp = max(max_temp),
    min_temp = min(min_temp),
    mean_windspeed = mean(mean_windspeed),
    max_windspeed = max(max_windspeed)
  )
df_2021_new$year = 2021
df_2021_new$Y <- ifelse(df_2021_new$total_rain > 0, 1, 0)

rm(df_2018)
rm(df_2019)
rm(df_2020)
rm(df_2021)
```

2021 data will be used for testing. 2019, 2019, 2020 data will be used for training
```{r}
df_train <- do.call(rbind, list(df_2018_new, df_2019_new, df_2020_new))
```

```{r}
df_monthly <- df_train %>% group_by(year, mon) %>%
  summarise(
    mean_rainfall = mean(total_rain)
  )
```


## Exploratory Data Analysis

maybe can check for: any high correlation? imbalanced data? 
```{r}
barplot(table(df_train$Y), xlab = 'Rainfall', ylab = 'Counts')

```

```{r}
table(df_train$Y)
```

```{r}
barplot(table(df_test$Y), xlab = 'Rainfall', ylab = 'Counts')

```

```{r, fig.height=10}
ts.plot(df_train$mean_rain)
```

```{r}
auto.arima
```

```{r, fig.height=6}
ts.plot(df_monthly$mean_rainfall)
```

```{r}
plot(df_monthly$mean_rainfall[df_monthly$year == 2018], type = 'l', col = 'blue',
     ylab = "Mean monthly rainfall",xlab = 'Months', ylim = c(0, 160))
lines(df_monthly$mean_rainfall[df_monthly$year == 2019], col = 'coral1')
lines(df_monthly$mean_rainfall[df_monthly$year == 2020], col= 'magenta1')
legend(10, 50, lty = 1, legend = c(2018, 2019, 2020), 
       col = c('blue', 'coral1', 'magenta1'), cex = 0.8)
```

```{r}
summary(df_train)
```

```{r}
# check for outliers
par(mfrow = c(2,1))
hist(df_train$mean_temp, xlab = "Mean temperature", main = "Histogram of mean temperature", breaks = 30)
hist(df_train$mean_windspeed, xlab = "Mean windspeed", main = "Histogram of mean windspeed", breaks = 30)

# we observe that there are some possible outliers for temperature, however this temperature are still well within the temperature range of Singapore. The lowest temperature Singapore has experienced is ... on ... Hence, we will not be removing any outliers
```

```{r}
#par(mfrow = c(2,1))
#hist(df_train$mean_rain, xlab = "Mean Rainfall", main = "Histogram of mean rainfall", breaks = 150)
hist(df_train$total_rain, xlab = "Total Rainfall", main = "Histogram of total rainfall", breaks = 150)

# not necessary outliers, still well within the total amount of rainfall experienced in singapore. the highest rainfall experienced in singapore was.. in...
```

```{r}
# correlation
cor_data <- cor(df_train[-c(11)])
cor_data[lower.tri(cor_data,diag=TRUE)] <- NA
cor_data <- as.data.frame(as.table(cor_data))
cor_data <- na.omit(cor_data)
cor_data <- cor_data[order(-abs(cor_data$Freq)),]

#cor_data2 <- subset(cor_data, abs(Freq) > 0.6)
#cor_data2

cor_data

# (total_rain, mean_rain), (mean_temp, max_temp) and (mean_temp, min_temp) seems to have high correlation
# however since, total_rain and mean rain are not used as predictors, we can ignore it
```

```{r}
# check for VIF values

library(MASS)

data.frame('columns' = colnames(df_train[,c(5,6,7)],),
           'VIF' = diag(ginv(cor(df_train[,c(5,6,7)]))))

# there are no high VIF values, hence multicollinearity is not present, we do not need to remove any variables?
```


## Models
```{r}
# we need to use previous day data's to predict today's weather

df_train2 <- df_train[-c(3, 4, 10)]
df_train2$Y <- as.factor(df_train2$Y)
df_train2[c(2:nrow(df_train2)),c(3:7)] <- df_train2[-c(nrow(df_train2)),c(3:7)]
df_train2 <- df_train2[-c(1),]


df_test <- df_2021_new[-c(3, 4, 10)]
df_test$Y <- as.factor(df_test$Y)
df_test[c(2:nrow(df_test)),c(3:7)] <- df_test[-c(nrow(df_test)),c(3:7)]
df_test <- df_test[-c(1),]
```


```{r}
# split to train and validation
# 2018 and 2019 used for train, 2020 used for validation

df_subtrain <- df_train[df_train$year != 2020,]
df_subtrain <- df_subtrain[-c(3, 4, 10)]
df_subtrain$Y <- as.factor(df_subtrain$Y)
df_subtrain[c(2:nrow(df_subtrain)),c(3:7)] <- df_subtrain[-c(nrow(df_subtrain)),c(3:7)]
df_subtrain <- df_subtrain[-c(1),]

df_val <- df_train[df_train$year == 2020,]
df_val <- df_val[-c(3, 4, 10)]
df_val$Y <- as.factor(df_val$Y)
df_val[c(2:nrow(df_val)),c(3:7)] <- df_val[-c(nrow(df_val)),c(3:7)]
df_val <- df_val[-c(1),]
```

```{r}
write.csv(df_train2, "C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/df_train2.csv", row.names = FALSE)

write.csv(df_subtrain, "C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/df_subtrain.csv", row.names = FALSE)

```

```{r}
# oversampled data
df_train2_os <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/df_train2_os.csv")
df_train2_os$Y <- as.factor(df_train2_os$Y)


df_subtrain_os <- read.csv("C:/Users/chuax/OneDrive - National University of Singapore/NUS/YEAR 3 SEM 2/ST4248/TERM PAPER/df_subtrain_os.csv")
df_subtrain_os$Y <- as.factor(df_subtrain_os$Y)
```


```{r}
# trying to oversample using OSTSC
OST
```


## Baseline model - Predict everything as rain
```{r}
# the accuracy achieved will be: 82.7%, hence our subsequent models will aim to do better than this.
301/(301+63)

table(df_test$Y)
```

## Logistic regression
```{r}
library(glmnet)
lm1 <- glm(Y~., data = df_train2, family = 'binomial')

summary(lm1)
```

```{r}
prob <- predict(lm1, newdata = df_test, type = 'response')
pred <- ifelse(prob > 0.5, 1, 0)

table(predict = pred, truth = df_test$Y)
```

```{r}
# accuracy
(6+294)/nrow(df_test)
```

```{r}
## removing the insignificant variables:
lm2 <- glm(Y~ day+mean_temp + max_temp + min_temp + mean_windspeed + max_windspeed, data = df_train2, family = 'binomial')

summary(lm2)
```

```{r}
prob <- predict(lm2, newdata = df_test, type = 'response')
pred <- ifelse(prob > 0.5, 1, 0)

table(predict = pred, truth = df_test$Y)
```

```{r}
# accuracy
(5 + 295)/nrow(df_test)

# we observe that logistic regression achieves a rather high accuracy, but it is still not better than the baseline model. Hence, for our subsequent models, we will try a more flexible model to see if the model does better
```

```{r}
# try with oversampled data
lm1 <- glm(Y~., data = df_train2_os, family = 'binomial')

summary(lm1)

```

```{r}
prob <- predict(lm1, newdata = df_test, type = 'response')
pred <- ifelse(prob > 0.5, 1, 0)

table(predict = pred, truth = df_test$Y)

```

```{r}
# accuracy
(43 + 235)/nrow(df_test)
```



## Decision Tree - CV on pruning
```{r}
library(tree)

tree.fit <- tree(Y ~., data = df_train2)

plot(tree.fit)
text(tree.fit, pretty = 0, cex = 0.75)
```

```{r}
tree.pred <- predict(tree.fit, newdata = df_test, type = 'class')

table(predict = tree.pred, truth = df_test$Y)

# predict most instances as rain, than not rain. Because there are more observations of raining.
```

```{r}
# try with oversampled data
tree.fit <- tree(Y ~., data = df_train2_os)

plot(tree.fit)
text(tree.fit, pretty = 0, cex = 0.75)
```

```{r}
tree.pred <- predict(tree.fit, newdata = df_test, type = 'class')

table(predict = tree.pred, truth = df_test$Y)

```

```{r}
(36+230)/nrow(df_test)
```




## Random Forest
```{r}
library(randomForest)
set.seed(1)

rf.fit <- randomForest(Y ~., data = df_subtrain, mtry = 3, importance = TRUE)

rf.pred <- predict(rf.fit, newdata = df_val, type = 'class')

table(predict = rf.pred, truth = df_val$Y)
```

```{r}
accuracy <- c()

# for loop to find the optimal k nearest neighbour
for (i in 1:7){
  set.seed(1)
  
  rf.fit <- randomForest(Y ~., data = df_subtrain, mtry = i, importance = TRUE)
  rf.pred <- predict(rf.fit, newdata = df_val, type = 'class')
  confusion_mat <- table(predict = rf.pred, truth = df_val$Y)

  accuracy[i] <- (confusion_mat[1,1]+confusion_mat[2,2])/nrow(df_val)

}
```

```{r}
plot(c(1:7), accuracy, type = 'b', xlab = 'mtry')
points(1, accuracy[1], pch = 19, col = 'red')
```

```{r}
set.seed(1)

rf.fit <- randomForest(Y ~., data = df_train2, mtry = 1, importance = TRUE)
rf.pred <- predict(rf.fit, newdata = df_test, type = 'class')

table(predict = rf.pred, truth = df_test$Y)

```

```{r}
# accuracy 
(4 + 299)/ nrow(df_test)

# slightly better than baseline model
```

```{r}
varImpPlot(rf.fit)
```


## XGBoost
```{r}
# pairwise
# cv on eta, gamma and max_depth

# eta: 0.2, 0.25, 0.3, 0.35
# gamma: 0, 0.01, 0.1, 0.2
# max_depth: 4, 5, 6, 7

combi <- c()

eta <- c(0.2, 0.25, 0.3, 0.35)
gam <- c(0, 0.01, 0.05, 0.1)
max_dep <- c(4,5,6,7)

idx <- 1

for (i in eta) {
  for (j in gam) {
    for (k in max_dep) {
      combi[[idx]] <- c(i,j,k)
      idx <- idx + 1
    }
  }
}
```

```{r}

accuracy <- c()
for (i in combi) {
  set.seed(1)
  xgb <- xgboost(data = as.matrix(df_subtrain[,-8]), label = as.numeric(df_subtrain$Y)-1, objective = 'binary:logistic',
                 nrounds = 10, eta = i[1], gamma = i[2], max_depth = i[3])
  
  pred <- predict(xgb, as.matrix(df_val[,-8]))
  pred2 <- ifelse(pred > 0.5, 1, 0)
  
  confusion_mat <- table(predict = pred2, truth = df_val$Y)
  accuracy <- c(accuracy, (confusion_mat[1,1]+confusion_mat[2,2])/nrow(df_val))

}
```

```{r}
which(accuracy == max(accuracy))

combi[[37]]
```

```{r}
accuracy <- c()

for (i in 2:20) {
  set.seed(1)
  xgb <- xgboost(data = as.matrix(df_subtrain[,-8]), label = as.numeric(df_subtrain$Y)-1, objective = 'binary:logistic',
                 nrounds = i, eta = 0.3, gamma = 0.01, max_depth = 4)
  
  pred <- predict(xgb, as.matrix(df_val[,-8]))
  pred2 <- ifelse(pred > 0.5, 1, 0)
  
  confusion_mat <- table(predict = pred2, truth = df_val$Y)
  accuracy <- c(accuracy, (confusion_mat[1,1]+confusion_mat[2,2])/nrow(df_val))

}
```

```{r}
plot(c(2:20), accuracy, type = 'b', xlab = 'nrounds')
points(9, accuracy[8], pch = 19, col = 'red')
```


```{r}
library(xgboost)

set.seed(1)
xgb <- xgboost(data = as.matrix(df_train2[,-8]), label = as.numeric(df_train2$Y)-1,
               objective = 'binary:logistic',
               nrounds = 9, eta = 0.3, gamma = 0.01, max_depth = 4)

pred <- predict(xgb, as.matrix(df_test[,-8]))
pred2 <- ifelse(pred > 0.5, 1, 0)

table(predict = pred2, truth = df_test$Y)

```

```{r}
# accuracy 
(16+290)/nrow(df_test)
```

```{r}
# with oversampled data
set.seed(1)
xgb <- xgboost(data = as.matrix(df_train2_os[,-8]), label = as.numeric(df_train2_os$Y)-1,
               objective = 'binary:logistic',
               nrounds = 9, eta = 0.3, gamma = 0, max_depth = 4)

pred <- predict(xgb, as.matrix(df_test[,-8]))
pred2 <- ifelse(pred > 0.5, 1, 0)

table(predict = pred2, truth = df_test$Y)

```



## SVM
```{r}
# tutorial 6
library(e1071)

seqq <- seq(0.01, 10, 0.5)
tune.out <- tune(svm, Y~., data = df_train2, scale = T, kernel = 'linear', 
                 ranges = list(cost = seqq))

summary(tune.out)
```

```{r}
bestmod <- tune.out$best.model

pred <- predict(bestmod, newdata = df_test, type = 'class')

table(predict = pred, truth = df_test$Y)

```


```{r}
seqq <- seq(0.01, 10, 0.5)
seqq2 <- c(0.5, 1, 2, 3, 4)
  
  
tune.out <- tune(svm, Y~., data = df_train2, scale = T, kernel = 'radial', 
                 ranges = list(cost = seqq, gamma = seqq2))
summary(tune.out)
```

```{r}
bestmod <- tune.out$best.model

pred <- predict(bestmod, newdata = df_test, type = 'class')

table(predict = pred, truth = df_test$Y)
```

```{r}
(2+294)/nrow(df_test)
```


maybe large variance in wind speed, shud try boosting/bagging methods to reduce variance

we can try oversampling to see if it helps in accuracy?